{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtR1xgooZ5l1"
   },
   "source": [
    "## **Evaluation Pipeline (Qwen2-Audio Baseline & CoT-Finetuned Model)**\n",
    "\n",
    "This notebook provides the evaluation workflow for comparing the original Qwen2-Audio-7B-Instruct model with our CoT fine-tuned version.\n",
    "Before running the pipeline, ensure that the model checkpoint(`MODEL_ID` = `\"Qwen/Qwen2-Audio-7B-Instruct\"`)has been downloaded locally.\n",
    "We evaluate both models end-to-end on `TEARS_V2`, using a unified pipeline that handles inference, label extraction, reasoning parsing, and dataset-level scoring.\n",
    "\n",
    "**1. Preparing the Environment**\n",
    "*   Make sure the checkpoint ZIP is already uploaded to Colab.\n",
    "*   The dataset ZIP must also be pre-uploaded.\n",
    "\n",
    "\n",
    "**2. Core Functions for Inference & Label Extraction**\n",
    "*   `predict_one()`: Runs inference for a single audio file.\n",
    "*   `extract_label()`: Parses the model's output text to obtain the predicted label. If the text contains \"most consistent with:\", extract everything after it. Otherwise, parse the last line and take the substring after the last \"is\".\n",
    "*   `eval_full_dataset()`: Outputs a .jsonl file where each line is one sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh9eQdNJqZVz",
    "outputId": "b0b0d85e-a1c6-4d74-e5b9-6a14b40ab60a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDlLn9FwqAOO"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install required packages\n",
    "# ============================================================\n",
    "!pip install -q transformers accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsFGD5ZqYZht"
   },
   "source": [
    "unzip kook's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_wW_Gq7YYlU",
    "outputId": "acf11ace-5475-46b7-bbb3-5e356f65c69d"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Unzip Model to Colab Local Disk\n",
    "# ============================================================\n",
    "\n",
    "ZIP_FILE = \"/content/drive/MyDrive/QwenFinetune/kook-checkpoint-9500.zip\"\n",
    "TARGET_DIR = \"/content/kook\"  # Extract to Colab local disk\n",
    "\n",
    "print(\"Unzipping to Colab local disk (faster)...\")\n",
    "print(f\"Source: {ZIP_FILE}\")\n",
    "print(f\"Target: {TARGET_DIR}\")\n",
    "\n",
    "!unzip -q \"{ZIP_FILE}\" -d \"{TARGET_DIR}\"\n",
    "\n",
    "print(\"\\n✓ Unzip completed!\")\n",
    "print(f\"Checkpoint is now at: {TARGET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVw3NlJO-KVu"
   },
   "source": [
    "Unzip dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KW_drAsd7Kcu",
    "outputId": "7229fa22-7d5a-4eec-9dc3-eb718c1adf76"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Unzip TEARS_V2 Dataset to Colab Local Disk\n",
    "# ============================================================\n",
    "\n",
    "ZIP_FILE = \"/content/drive/MyDrive/Tears_v2.zip\"\n",
    "TARGET_DIR = \"/content/TEARS_V2\"  # Extract to Colab local disk\n",
    "\n",
    "print(\"Unzipping to Colab local disk (faster)...\")\n",
    "print(f\"Source: {ZIP_FILE}\")\n",
    "print(f\"Target: {TARGET_DIR}\")\n",
    "print(\"\\nThis will take approximately 5-10 minutes for 6GB...\")\n",
    "\n",
    "!unzip -q \"{ZIP_FILE}\" -d \"{TARGET_DIR}\"\n",
    "\n",
    "print(\"\\n✓ Unzip completed!\")\n",
    "print(f\"Dataset is now at: {TARGET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSDhJEa_iVOG"
   },
   "source": [
    "Update package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dMeV56-bDNH",
    "outputId": "a9b83255-54a2-4fcf-c909-6c0c82782935"
   },
   "outputs": [],
   "source": [
    "!pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFx5LnS99AmN"
   },
   "outputs": [],
   "source": [
    "!pip install -q \"transformers>=4.45.0\" \"datasets\" \"peft\" \"accelerate\" \\\n",
    "               \"soundfile\" \"librosa\" wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXUv5kwSA71s",
    "outputId": "85445322-b8a0-4e4f-b638-b25a1ee73eaa"
   },
   "outputs": [],
   "source": [
    "# Uninstall current version and install a working version\n",
    "!pip uninstall transformers -y\n",
    "!pip install transformers==4.57.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qO_koE_KmdK-"
   },
   "source": [
    "filter test data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyq0MHokmch4",
    "outputId": "df4b2785-cec3-42e1-e572-7bcd8904d596"
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# in_path = \"/content/TEARS_V2/TEARS_V2/test_v3.json\"\n",
    "# out_path = \"/content/drive/MyDrive/test_only_v3.json\" # /content/drive/MyDrive\n",
    "\n",
    "# with open(in_path, \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# test_data = [\n",
    "#     item for item in data\n",
    "#     if \"/test/\" in item.get(\"audio_path\", \"\").lower()\n",
    "# ]\n",
    "\n",
    "# print(f\"Total samples: {len(data)}, test samples: {len(test_data)}\")\n",
    "\n",
    "# with open(out_path, \"w\") as f:\n",
    "#     json.dump(test_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpgCQ-Tu9vNr"
   },
   "source": [
    "Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adOPbizUqDaX"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Import necessary libraries\n",
    "# ============================================================\n",
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oileLxewA6-K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Qwen2AudioForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjrT_dMF1mRV"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from transformers import AutoProcessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eu-mHCl1A-kF"
   },
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Configuration\n",
    "# ==========================\n",
    "from peft import PeftModel\n",
    "DATA_ROOT = Path(\"/content/drive/MyDrive\")\n",
    "TEST_FILE = str(DATA_ROOT / \"test_only_v3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnvGpuhywVw8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t87aLwYqKP6S"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X54v8-jKRFz"
   },
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def predict_dialect(audio_path: str, prompt: str, max_new_tokens: int = 64):\n",
    "#     audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"audio\", \"audio\": audio, \"sampling_rate\": sr},\n",
    "#                 {\"type\": \"text\", \"text\": prompt},\n",
    "#             ],\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "#     text = processor.apply_chat_template(\n",
    "#         messages,\n",
    "#         add_generation_prompt=True,\n",
    "#         tokenize=False,\n",
    "#     )\n",
    "\n",
    "#     inputs = processor(\n",
    "#         text=[text],\n",
    "#         audios=[audio],\n",
    "#         sampling_rate=sr,\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#     ).to(device)\n",
    "\n",
    "#     # output\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             do_sample=False,\n",
    "#             temperature=0.0,\n",
    "#         )\n",
    "\n",
    "#     # decode\n",
    "#     output_text = processor.batch_decode(\n",
    "#         generated_ids[:, inputs[\"input_ids\"].shape[-1]:],\n",
    "#         skip_special_tokens=True,\n",
    "#     )[0]\n",
    "\n",
    "#     return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1fHorOLangv"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "\n",
    "def predict_one(audio_path, prompt, max_new_tokens=96):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": audio, \"sampling_rate\": sr},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        audio=[audio],\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "\n",
    "    new_tokens = gen_ids[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "    output = processor.batch_decode(\n",
    "        new_tokens,\n",
    "        skip_special_tokens=True,\n",
    "    )[0]\n",
    "\n",
    "    return output.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5KfB-yXauCM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "TEST_JSON = TEST_FILE\n",
    "\n",
    "with open(TEST_JSON, \"r\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXr8xhVwpadf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(11785)\n",
    "AUDIO_ROOT = Path(\"/content/TEARS_V2/TEARS_V2\")\n",
    "\n",
    "def extract_label(text: str) -> str:\n",
    "    \"\"\"\n",
    "    extract label from generated sentences\n",
    "    if contains 'most consistent with:'，else use the last line after 'is'\n",
    "    \"\"\"\n",
    "    t = text.strip()\n",
    "    lower = t.lower()\n",
    "\n",
    "    if \"most consistent with:\" in lower:\n",
    "        idx = lower.rfind(\"most consistent with:\")\n",
    "        label_part = t[idx + len(\"most consistent with:\") :]\n",
    "    else:\n",
    "        last_line = t.splitlines()[-1].strip()\n",
    "        last_lower = last_line.lower()\n",
    "\n",
    "        is_idx = last_lower.rfind(\" is \")\n",
    "        if is_idx != -1:\n",
    "            label_part = last_line[is_idx + len(\" is \") :]\n",
    "        else:\n",
    "            label_part = last_line\n",
    "\n",
    "    label = label_part.strip().strip(\".\")\n",
    "    return label.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulQe8lj388W3"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_reasoning(gt_reasoning: str, pred_text: str):\n",
    "    \"\"\"\n",
    "    record questionable points in reasoning by string match\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    gt = gt_reasoning.lower()\n",
    "    pred = pred_text.lower()\n",
    "\n",
    "    # 1) apparently opposite acoustic description\n",
    "    opposite_pairs = [\n",
    "        (\"high rhoticity\", \"low rhoticity\"),\n",
    "        (\"low rhoticity\", \"high rhoticity\"),\n",
    "        (\"high pitch\", \"low pitch\"),\n",
    "        (\"low pitch\", \"high pitch\"),\n",
    "        (\"high vowel reduction\", \"low vowel reduction\"),\n",
    "        (\"low vowel reduction\", \"high vowel reduction\"),\n",
    "    ]\n",
    "    for gt_phrase, pred_phrase in opposite_pairs:\n",
    "        if gt_phrase in gt and pred_phrase in pred:\n",
    "            issues.append(f\"GT has '{gt_phrase}' but prediction says '{pred_phrase}'\")\n",
    "\n",
    "    # 2) Obvious social attribute hallucination\n",
    "    if \"biracial\" in pred:\n",
    "        issues.append(\"Mentions 'biracial' (not annotated in ground-truth labels).\")\n",
    "    if \"jianghuai mandarin\" in pred or \"hefei dialect\" in pred:\n",
    "        issues.append(\"Mentions non-existent dialect label (e.g., Jianghuai Mandarin).\")\n",
    "\n",
    "    return issues\n",
    "\n",
    "def eval_random_k(samples, k=50):\n",
    "    picked = random.sample(samples, k)\n",
    "    correct = 0\n",
    "    for i, ex in enumerate(picked, 1):\n",
    "        audio = AUDIO_ROOT / ex[\"audio_path\"]\n",
    "        prompt = ex[\"prompt\"]\n",
    "        gt_label = ex[\"answer\"].strip().lower()\n",
    "        pred = predict_one(audio, prompt)\n",
    "        pred_label = extract_label(pred)\n",
    "        is_correct = (pred_label == gt_label)\n",
    "        correct += int(is_correct)\n",
    "        print(f\"\\n=== Sample {i} ===\")\n",
    "        print(f\"Audio: {audio}\")\n",
    "        print(f\"Question: {prompt}\")\n",
    "        print(f\"GT label: {gt_label}\")\n",
    "        print(f\"Pred label: {pred_label}\")\n",
    "        print(f\"Full GT: {ex['response']}\")\n",
    "        print(f\"Generated: {pred}\")\n",
    "        print(f\"Correct: {is_correct}\")\n",
    "    acc = correct / k\n",
    "    print(f\"\\nRandom-{k} Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "def eval_full_dataset(samples, out_path: str = \"eval_full.jsonl\", verbose: bool = False):\n",
    "    n = len(samples)\n",
    "    # picked = random.sample(samples, k)\n",
    "    correct = 0\n",
    "    out_f = open(out_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(samples, desc=\"Eval full dataset\")):\n",
    "        audio = AUDIO_ROOT / ex[\"audio_path\"]\n",
    "        prompt = ex[\"prompt\"]\n",
    "        gt_label = ex[\"answer\"].strip().lower()\n",
    "\n",
    "        pred = predict_one(audio, prompt)\n",
    "        pred_label = extract_label(pred)\n",
    "\n",
    "        is_correct = (pred_label == gt_label)\n",
    "        correct += int(is_correct)\n",
    "\n",
    "        # reasoning_issues = analyze_reasoning(ex[\"response\"], pred)\n",
    "\n",
    "        record = {\n",
    "                \"idx\": i,\n",
    "                \"audio_path\": str(audio),\n",
    "                \"question\": prompt,\n",
    "                \"gt_label\": gt_label,\n",
    "                \"pred_label\": pred_label,\n",
    "                \"correct\": bool(is_correct),\n",
    "                \"gt_reasoning\": ex[\"response\"],\n",
    "                \"pred_text\": pred,\n",
    "                # \"reasoning_issues\": reasoning_issues,\n",
    "            }\n",
    "        out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Sample {i} ===\")\n",
    "            print(f\"Audio: {audio}\")\n",
    "            print(f\"Question: {prompt}\")\n",
    "            print(f\"GT label: {gt_label}\")\n",
    "            print(f\"Pred label: {pred_label}\")\n",
    "            print(f\"Full GT: {ex['response']}\")\n",
    "            print(f\"Generated: {pred}\")\n",
    "            print(f\"Correct: {is_correct}\")\n",
    "            # if reasoning_issues:\n",
    "            #     print(f\"Reasoning issues: {reasoning_issues}\")\n",
    "\n",
    "    out_f.close()\n",
    "    acc = correct / n if n > 0 else 0.0\n",
    "    print(f\"\\nFull-dataset Accuracy ({n} samples): {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcOCwTNkqnxC"
   },
   "source": [
    "Evaluate original Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmt0qhPp8yE_"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "5fdf54086abe4e36b9794d31a3be83c2",
      "f8ff91e8e09a4f2eabb1f4377c7e931b",
      "0f10ae9f9210443c8f0fa3406aecce78",
      "f674ec868a444968a46beb0b97773f04",
      "1bd5e5dcb43540ebb5043a5f8e9189bb",
      "53d384f11c1f4e3080a585cf6294d120",
      "4fb01d8851c0457a9eae66879087d9ac",
      "e243869a1b244d008ef826c3cfb398d1",
      "a904e68f57ae4626acf24dda3534c0ae",
      "e17906027c954bbe8c49eb09e6aa3e46",
      "ffe08a805d244057b187a2de87b20a29"
     ]
    },
    "id": "iJ7BOm48qnMj",
    "outputId": "bafc2e7e-84c3-454f-87b2-bf7a9cc6cbb0"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"/content/drive/MyDrive/QwenFinetune/Qwen2-Audio-7B-Instruct\"\n",
    "ADAPTER_DIR = \"/content/kook\" # TODO: test kook-checkpoint-9500\n",
    "\n",
    "print(f\"Loading model and processor from: {BASE_MODEL}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Load original model...\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rSG5-4Z-7NC",
    "outputId": "0b0b6cf9-4951-4d56-febd-14de4e59ab0a"
   },
   "outputs": [],
   "source": [
    "eval_random_k(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FWxeWcb0qtVf",
    "outputId": "0e3b2eea-c04d-41ef-af0f-3f067c1bf9f1"
   },
   "outputs": [],
   "source": [
    "eval_full_dataset(test_data, \"/content/drive/MyDrive/eval_full.jsonl\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFtIf5UaqSk1"
   },
   "source": [
    "Evaluate finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "c532ca69cf744ebb8acece216630f5c4",
      "8334c741dbcd4cfe81cebb8903ff72a8",
      "65af5be1167443818956fb417f5f3d33",
      "3ad5a8b3acfa43ee88533978effeda50",
      "7cd8ea2cf4d44c7088cd286ace6f6b71",
      "c395b49473974c7cac871cd3944dc43a",
      "84834b3f2347407f88328c5c29e529b8",
      "525008ca15854a01a2deb9b2cb4cec30",
      "aaa340aeffda48bfb0f32b6c50147002",
      "dd592cd05fa74bcab087e02e5a4e9f85",
      "0844b6eb29ac47e5b03345edb0bdd719"
     ]
    },
    "id": "ZZVBoi6fZBoC",
    "outputId": "5cfe51ec-d637-40b5-df22-bb257b1d4a13"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"/content/drive/MyDrive/QwenFinetune/Qwen2-Audio-7B-Instruct\"\n",
    "ADAPTER_DIR = \"/content/kook\" # TODO: test kook-checkpoint-9500\n",
    "\n",
    "print(f\"Loading model and processor from: {BASE_MODEL}\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Load base model...\")\n",
    "base_model = Qwen2AudioForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"Load LoRA adapter from: {ADAPTER_DIR}\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ADAPTER_DIR,\n",
    "    is_trainable=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Model and processor loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNS3cVGyA5_k",
    "outputId": "1f6149d1-71f7-41b7-8081-8a30e84214d3"
   },
   "outputs": [],
   "source": [
    "eval_random_k(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jS0W-WAqQf3",
    "outputId": "051c6ed1-7a0e-41a5-888c-ace49343f54d"
   },
   "outputs": [],
   "source": [
    "eval_full_dataset(test_data, \"/content/drive/MyDrive/eval_full_finetuned.jsonl\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlGLV2HqXX7-",
    "outputId": "f52c414f-1dee-4ea9-ab39-e962d1f95e3f"
   },
   "outputs": [],
   "source": [
    "# audio_path = \"/content/TEARS_V2/TEARS_V2/ears_dataset/val/p101/freeform_speech_03_640000_800000.wav\"\n",
    "# prompt = \"What is the speaker's ethnicity?\"\n",
    "\n",
    "# print(predict_dialect(audio_path, prompt)) # using current model\n",
    "\n",
    "# # {\n",
    "# #     \"audio_path\": \"ears_dataset/val/p101/freeform_speech_03_640000_800000.wav\",\n",
    "# #     \"prompt\": \"What is the speaker's ethnicity?\",\n",
    "# #     \"response\": \"Because of the combination of high rhoticity_ratio, presence of syllabic_consonants, and vowel articulation patterns consistent with Southern American English, the speaker is most consistent with: black or african american\",\n",
    "# #     \"answer\": \"black or african american\"\n",
    "# # }"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
